{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b982558-2b21-4e4c-ba69-8f57e8073088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path = /home/sohithvishnu/Documents/AI_Project/habitat-sim/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohithvishnu/anaconda3/envs/habitat/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import git\n",
    "import imageio\n",
    "import magnum as mn\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# function to display the topdown map\n",
    "from PIL import Image\n",
    "\n",
    "import habitat_sim\n",
    "from habitat_sim.utils import common as utils\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "dir_path = repo.working_tree_dir\n",
    "data_path = os.path.join(dir_path, \"data\")  # Please check data folder, before you intialize the code\n",
    "print(f\"data_path = {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7105eba-180e-4466-a019-4800eda0bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PluginManager::Manager: duplicate static plugin StbImageImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin GltfImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin BasisImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AssimpImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AnySceneImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AnyImageImporter, ignoring\n"
     ]
    }
   ],
   "source": [
    "# @title Define Observation Display Utility Function { display-mode: \"form\" }\n",
    "\n",
    "# @markdown A convenient function that displays sensor observations with matplotlib.\n",
    "\n",
    "# @markdown (double click to see the code)\n",
    "\n",
    "\n",
    "# Change to do something like this maybe: https://stackoverflow.com/a/41432704\n",
    "def display_sample(rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([])):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--no-display\", dest=\"display\", action=\"store_false\")\n",
    "    parser.add_argument(\"--no-make-video\", dest=\"make_video\", action=\"store_false\")\n",
    "    parser.set_defaults(show_video=True, make_video=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    show_video = args.display\n",
    "    display = args.display\n",
    "    do_make_video = args.make_video\n",
    "else:\n",
    "    show_video = False\n",
    "    do_make_video = False\n",
    "    display = False\n",
    "\n",
    "# import the maps module alone for topdown mapping\n",
    "if display:\n",
    "    from habitat.utils.visualizations import maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d32cff-d249-48ed-a8ff-f78e2f481ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the scene we are going to load.\n",
    "# we support a variety of mesh formats, such as .glb, .gltf, .obj, .ply\n",
    "test_scene = os.path.join(\n",
    "    data_path, \"scene_datasets/mp3d/17DRP5sb8fy/17DRP5sb8fy.glb\" # This is data input part, please change it. \n",
    ")                                                                # You can download from the data link in the readme. \n",
    "\n",
    "sim_settings = {\n",
    "    \"scene\": test_scene,  # Scene path\n",
    "    \"default_agent\": 0,  # Index of the default agent\n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters, relative to the agent\n",
    "    \"width\": 256,  # Spatial resolution of the observations\n",
    "    \"height\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4001386-c5b4-4a6e-89ee-d48620e42395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates a config for the simulator.\n",
    "# It contains two parts:\n",
    "# one for the simulator backend\n",
    "# one for the agent, where you can attach a bunch of sensors\n",
    "def make_simple_cfg(settings):\n",
    "    # simulator backend\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "\n",
    "    # agent\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "\n",
    "    # In the 1st example, we attach only one sensor,\n",
    "    # a RGB visual sensor, to the agent\n",
    "    rgb_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    rgb_sensor_spec.uuid = \"color_sensor\"\n",
    "    rgb_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    rgb_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    rgb_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "\n",
    "    agent_cfg.sensor_specifications = [rgb_sensor_spec]\n",
    "\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])\n",
    "\n",
    "\n",
    "cfg = make_simple_cfg(sim_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ba2fd0-f039-4a66-b5e4-ecc345382bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renderer: NVIDIA GeForce RTX 3060 Laptop GPU/PCIe/SSE2 by NVIDIA Corporation\n",
      "OpenGL version: 4.6.0 NVIDIA 560.35.03\n",
      "Using optional features:\n",
      "    GL_ARB_vertex_array_object\n",
      "    GL_ARB_separate_shader_objects\n",
      "    GL_ARB_robustness\n",
      "    GL_ARB_texture_storage\n",
      "    GL_ARB_texture_view\n",
      "    GL_ARB_framebuffer_no_attachments\n",
      "    GL_ARB_invalidate_subdata\n",
      "    GL_ARB_texture_storage_multisample\n",
      "    GL_ARB_multi_bind\n",
      "    GL_ARB_direct_state_access\n",
      "    GL_ARB_get_texture_sub_image\n",
      "    GL_ARB_texture_filter_anisotropic\n",
      "    GL_KHR_debug\n",
      "    GL_KHR_parallel_shader_compile\n",
      "    GL_NV_depth_buffer_float\n",
      "Using driver workarounds:\n",
      "    no-forward-compatible-core-context\n",
      "    nv-egl-incorrect-gl11-function-pointers\n",
      "    no-layout-qualifiers-on-old-glsl\n",
      "    nv-zero-context-profile-mask\n",
      "    nv-implementation-color-read-format-dsa-broken\n",
      "    nv-cubemap-inconsistent-compressed-image-size\n",
      "    nv-cubemap-broken-full-compressed-image-query\n",
      "    nv-compressed-block-size-in-bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PluginManager::Manager: duplicate static plugin StbImageImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin GltfImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin BasisImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AssimpImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AnySceneImporter, ignoring\n",
      "PluginManager::Manager: duplicate static plugin AnyImageImporter, ignoring\n",
      "[13:14:21:444494]:[Warning]:[Metadata] SceneDatasetAttributes.cpp(107)::addNewSceneInstanceToDataset : Dataset : 'default' : Lighting Layout Attributes 'no_lights' specified in Scene Attributes but does not exist in dataset, so creating default.\n"
     ]
    }
   ],
   "source": [
    "try:  # Needed to handle out of order cell run in Jupyter\n",
    "    sim.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sim = habitat_sim.Simulator(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e3e139-6f1b-415a-9654-a72923ea603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete action space:  ['move_forward', 'turn_left', 'turn_right']\n",
      "Combined video saved as 'combined_agent_paths.avi'.\n"
     ]
    }
   ],
   "source": [
    "import habitat_sim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # For video processing\n",
    "import random  # For generating random actions\n",
    "\n",
    "# Assuming sim and sim_settings are already defined, with a valid config\n",
    "\n",
    "# Get the discrete actions for the agent (move_forward, turn_left, turn_right)\n",
    "action_names = list(cfg.agents[sim_settings[\"default_agent\"]].action_space.keys())\n",
    "print(\"Discrete action space: \", action_names)\n",
    "\n",
    "# Number of agents\n",
    "num_agents = 2\n",
    "\n",
    "# Initialize the agents' states with different positions\n",
    "starting_positions = [\n",
    "    np.array([0.0,0.0,0.0]),  # Starting position for agent 0\n",
    "    np.array([1.0, 1.0, 0.0])   # Starting position for agent 1\n",
    "]\n",
    "\n",
    "# Initialize and store states for both agents and their paths\n",
    "agents_states = []\n",
    "agents_paths = [[] for _ in range(num_agents)]  # List of paths for each agent\n",
    "for i in range(num_agents):\n",
    "    agent = sim.initialize_agent(sim_settings[\"default_agent\"])\n",
    "    agent_state = habitat_sim.AgentState()\n",
    "    agent_state.position = starting_positions[i]\n",
    "    agent.set_state(agent_state)  # Set the initial state of the agent\n",
    "    agents_states.append(agent_state)  # Store the agent's state\n",
    "\n",
    "# Define a large number of random actions for each agent\n",
    "num_steps = 100  # Set the number of steps for each agent\n",
    "actions_per_agent = [\n",
    "    [random.choice(action_names) for _ in range(num_steps)] for _ in range(num_agents)\n",
    "]\n",
    "\n",
    "# Video writer setup\n",
    "frame_width, frame_height = 640, 480  # Adjust according to your observation size\n",
    "combined_frame_width = frame_width * 3  # Two agent views + top-down map\n",
    "out = cv2.VideoWriter('combined_agent_paths.avi', cv2.VideoWriter_fourcc(*'XVID'), 20.0, (combined_frame_width, frame_height))\n",
    "\n",
    "# convert 3d points to 2d topdown coordinates\n",
    "def convert_points_to_topdown(pathfinder, points, meters_per_pixel):\n",
    "    points_topdown = []\n",
    "    bounds = pathfinder.get_bounds()\n",
    "    for point in points:\n",
    "        # convert 3D x,z to topdown x,y\n",
    "        px = (point[0] - bounds[0][0]) / meters_per_pixel\n",
    "        py = (point[2] - bounds[0][2]) / meters_per_pixel\n",
    "        points_topdown.append(np.array([px, py]))\n",
    "    return points_topdown\n",
    "# Updated top-down map generation function\n",
    "def generate_topdown_map(agents_paths, height, meters_per_pixel=0.1):\n",
    "    # Convert points to top-down map visualization\n",
    "    top_down_map = maps.get_topdown_map(sim.pathfinder, height=height, meters_per_pixel=meters_per_pixel)\n",
    "    recolor_map = np.array([[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8)\n",
    "    top_down_map = recolor_map[top_down_map]\n",
    "\n",
    "    # Overlay each agent's path and current position on the top-down map\n",
    "    colors = [(255, 0, 0), (0, 0, 255)]  # Red for agent 0, blue for agent 1\n",
    "    for i, agent_path in enumerate(agents_paths):\n",
    "        if len(agent_path) > 1:\n",
    "            # Draw the path as a line connecting all positions\n",
    "            for j in range(1, len(agent_path)):\n",
    "                start_pos = tuple(map(int, convert_points_to_topdown(sim.pathfinder, [agent_path[j - 1]], meters_per_pixel)[0]))\n",
    "                end_pos = tuple(map(int, convert_points_to_topdown(sim.pathfinder, [agent_path[j]], meters_per_pixel)[0]))\n",
    "                cv2.line(top_down_map, start_pos, end_pos, colors[i], thickness=2)\n",
    "\n",
    "        # Draw the current position as a dot\n",
    "        current_pos = tuple(map(int, convert_points_to_topdown(sim.pathfinder, [agent_path[-1]], meters_per_pixel)[0]))\n",
    "        cv2.circle(top_down_map, current_pos, 5, colors[i], -1)  # -1 fills the circle\n",
    "\n",
    "    return top_down_map\n",
    "\n",
    "# Function to execute actions for each agent and capture frames\n",
    "def navigate_and_see(agent_index, action):\n",
    "    if action in action_names:\n",
    "        # Restore the agent's state before performing the action\n",
    "        agent_state = agents_states[agent_index]\n",
    "        agent = sim.initialize_agent(sim_settings[\"default_agent\"])\n",
    "        agent.set_state(agent_state)\n",
    "        \n",
    "        # Perform the action and get the observations\n",
    "        observations = sim.step(action)\n",
    "        \n",
    "        # Store the updated agent state after the action\n",
    "        agent_state = agent.get_state()\n",
    "        agents_states[agent_index] = agent_state\n",
    "        \n",
    "        # Store the current position in the agent's path\n",
    "        agents_paths[agent_index].append(agent_state.position.copy())\n",
    "        \n",
    "        # Capture RGB observation\n",
    "        rgb_image = observations[\"color_sensor\"]  # Get the RGB image from observations\n",
    "        rgb_image_bgr = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)  # Convert from RGB to BGR for OpenCV\n",
    "        return rgb_image_bgr\n",
    "\n",
    "# Main loop to execute actions for both agents and build video frames\n",
    "for step in range(num_steps):\n",
    "    agent_frames = []\n",
    "    for i in range(num_agents):\n",
    "        action = actions_per_agent[i][step]\n",
    "        agent_frame = navigate_and_see(i, action)\n",
    "        agent_frames.append(agent_frame)\n",
    "\n",
    "    # Generate the top-down map with updated agent paths\n",
    "    top_down_map_frame = generate_topdown_map(agents_paths, starting_positions[0][1])\n",
    "\n",
    "    # Ensure the top-down map has the same height as agent frames\n",
    "    top_down_map_frame = cv2.resize(top_down_map_frame, (frame_width, frame_height))\n",
    "\n",
    "    # Resize agent frames if necessary to match height\n",
    "    for idx in range(len(agent_frames)):\n",
    "        if agent_frames[idx].shape[:2] != (frame_height, frame_width):\n",
    "            agent_frames[idx] = cv2.resize(agent_frames[idx], (frame_width, frame_height))\n",
    "\n",
    "    # Combine all frames horizontally\n",
    "    combined_frame = np.hstack((agent_frames[0], agent_frames[1], top_down_map_frame))\n",
    "\n",
    "    # Write the combined frame to the video\n",
    "    out.write(combined_frame)\n",
    "\n",
    "# Release video writer\n",
    "out.release()\n",
    "print(\"Combined video saved as 'combined_agent_paths.avi'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75df137-2628-43f8-99bd-edad509ae5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
